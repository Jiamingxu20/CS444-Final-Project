2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_setup.py:_flush():67] Current SDK version is 0.19.7
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_setup.py:_flush():67] Configure stats pid to 4127593
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_setup.py:_flush():67] Loading settings from /u/yzhang70/.config/wandb/settings
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_setup.py:_flush():67] Loading settings from /u/yzhang70/CS444-Final-Project/wandb/settings
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_setup.py:_flush():67] Loading settings from environment variables
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_init.py:setup_run_log_directory():647] Logging user logs to /u/yzhang70/CS444-Final-Project/wandb/run-20250511_114332-9jugrpd0/logs/debug.log
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_init.py:setup_run_log_directory():648] Logging internal logs to /u/yzhang70/CS444-Final-Project/wandb/run-20250511_114332-9jugrpd0/logs/debug-internal.log
2025-05-11 11:43:32,069 INFO    MainThread:4127593 [wandb_init.py:init():761] calling init triggers
2025-05-11 11:43:32,070 INFO    MainThread:4127593 [wandb_init.py:init():766] wandb.init called with sweep_config: {}
config: {'model': 'vit_b_16', 'batch_size': 32, 'dropout_rate': 0.4, 'num_epochs': 500, 'train_set_ratio': 0.9, 'test_set_ratio': 0.09999999999999998, 'pretrained': True, '_wandb': {}}
2025-05-11 11:43:32,070 INFO    MainThread:4127593 [wandb_init.py:init():784] starting backend
2025-05-11 11:43:32,070 INFO    MainThread:4127593 [wandb_init.py:init():788] sending inform_init request
2025-05-11 11:43:32,076 INFO    MainThread:4127593 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-05-11 11:43:32,076 INFO    MainThread:4127593 [wandb_init.py:init():803] backend started and connected
2025-05-11 11:43:32,077 INFO    MainThread:4127593 [wandb_init.py:init():896] updated telemetry
2025-05-11 11:43:32,090 INFO    MainThread:4127593 [wandb_init.py:init():920] communicating run to backend with 90.0 second timeout
2025-05-11 11:43:32,337 INFO    MainThread:4127593 [wandb_init.py:init():995] starting run threads in backend
2025-05-11 11:43:32,641 INFO    MainThread:4127593 [wandb_run.py:_console_start():2377] atexit reg
2025-05-11 11:43:32,642 INFO    MainThread:4127593 [wandb_run.py:_redirect():2227] redirect: wrap_raw
2025-05-11 11:43:32,642 INFO    MainThread:4127593 [wandb_run.py:_redirect():2292] Wrapping output streams.
2025-05-11 11:43:32,642 INFO    MainThread:4127593 [wandb_run.py:_redirect():2317] Redirects installed.
2025-05-11 11:43:32,646 INFO    MainThread:4127593 [wandb_init.py:init():1037] run started, returning control to user process
2025-05-11 11:43:34,487 INFO    MainThread:4127593 [wandb_run.py:_config_callback():1261] config_cb None None {'net_summary': "VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Sequential(\n    (0): Dropout(p=0.4, inplace=False)\n    (1): Linear(in_features=768, out_features=315, bias=True)\n  )\n)"}
2025-05-11 11:43:34,487 INFO    MainThread:4127593 [wandb_watch.py:_watch():71] Watching
2025-05-11 12:07:31,844 WARNING MsgRouterThr:4127593 [router.py:message_loop():77] message_loop has been closed
Traceback (most recent call last):
  File "/u/yzhang70/miniconda3/envs/pinn/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py", line 28, in _read_message
    resp = self._sock_client.read_server_response(timeout=1)
  File "/u/yzhang70/miniconda3/envs/pinn/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 285, in read_server_response
    data = self._read_packet_bytes(timeout=timeout)
  File "/u/yzhang70/miniconda3/envs/pinn/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 270, in _read_packet_bytes
    raise SockClientClosedError
wandb.sdk.lib.sock_client.SockClientClosedError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/u/yzhang70/miniconda3/envs/pinn/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 70, in message_loop
    msg = self._read_message()
  File "/u/yzhang70/miniconda3/envs/pinn/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py", line 30, in _read_message
    raise MessageRouterClosedError from e
wandb.sdk.interface.router.MessageRouterClosedError
2025-05-11 12:07:31,858 INFO    MsgRouterThr:4127593 [mailbox.py:close():115] Closing mailbox, abandoning 1 handles.
